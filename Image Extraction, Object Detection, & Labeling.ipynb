{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1I-v-uSF1SR8dOzaNMdeT80t7Tnfq1HZc","timestamp":1749834719685}],"authorship_tag":"ABX9TyO0oWbLPaMg+SOuilazmB+d"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 1) Extract Video Stills"],"metadata":{"id":"5FmghcqzHyY7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"l8it6gaWVGPL"},"outputs":[],"source":["# Section 1) Extract Video Stills\n","\n","# === Import Libraries ===\n","\n","from pathlib import Path\n","import cv2\n","from google.colab import drive\n","\n","# === Configuration ===\n","INPUT_DIR = Path('/content/drive/MyDrive/FreeFuse_Project/Source_Videos/Alloy Personal Training')\n","OUTPUT_DIR = Path('/content/drive/MyDrive/FreeFuse_Project/Extracted_Stills')\n","CAPTURE_INTERVAL_S = 3      # seconds between captures\n","START_TIME_S = 1            # skip first N seconds of each video\n","VIDEO_EXTS = {'.mp4', '.mov', '.avi'}\n","\n","# === Functions ===\n","def mount_drive():\n","    \"\"\"Mount Google Drive to /content/drive\"\"\"\n","    print(\"Mounting Google Drive...\")\n","    drive.mount('/content/drive', force_remount=True)\n","    print(\"Drive mounted.\")\n","\n","def extract_stills(input_dir: Path, output_dir: Path, interval_s: float, start_s: float):\n","    \"\"\"\n","    Extract still frames from all videos in input_dir at every interval_s seconds,\n","    starting after start_s seconds, saving to output_dir.\n","    \"\"\"\n","    output_dir.mkdir(parents=True, exist_ok=True)\n","    video_files = [f for f in input_dir.iterdir() if f.suffix.lower() in VIDEO_EXTS]\n","\n","    if not video_files:\n","        print(f\"No videos found in {input_dir}\")\n","        return\n","\n","    print(f\"Found {len(video_files)} videos in {input_dir}\\n\")\n","    for idx, video_file in enumerate(video_files, start=1):\n","        print(f\"[{idx}/{len(video_files)}] Processing '{video_file.name}'\")\n","        cap = cv2.VideoCapture(str(video_file))\n","        if not cap.isOpened():\n","            print(f\"  ✗ Could not open {video_file.name}, skipping.\")\n","            continue\n","\n","        fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n","        start_frame = int(start_s * fps)\n","        if start_frame > 0:\n","            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n","            print(f\"  → Skipped first {start_s}s ({start_frame} frames)\")\n","\n","        next_capture = start_s\n","        saved_count = 0\n","        while True:\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            current_s = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0\n","            if current_s >= next_capture:\n","                mm = int(current_s // 60)\n","                ss = int(current_s % 60)\n","                timestamp = f\"{mm:02d}{ss:02d}\"\n","                out_name = f\"{video_file.stem}_{timestamp}.jpg\"\n","                out_path = output_dir / out_name\n","                cv2.imwrite(str(out_path), frame)\n","                print(f\"  ✔ Saved frame '{out_name}'\")\n","                saved_count += 1\n","                next_capture += interval_s\n","\n","        cap.release()\n","        print(f\"  Completed '{video_file.name}', saved {saved_count} frames.\\n\")\n","\n","    print(\"All videos processed.\")\n","\n","# === Main Execution ===\n","mount_drive()\n","extract_stills(INPUT_DIR, OUTPUT_DIR, CAPTURE_INTERVAL_S, START_TIME_S)"]},{"cell_type":"markdown","source":["## 2) Object Detection"],"metadata":{"id":"a_9DCsIuJdxm"}},{"cell_type":"markdown","source":["### 2a) DETR ResNet v50"],"metadata":{"id":"pp5I5I5aJkEV"}},{"cell_type":"code","source":["from pathlib import Path\n","import torch\n","import pandas as pd\n","from PIL import Image\n","import os\n","from transformers import AutoImageProcessor, AutoModelForObjectDetection\n","from google.colab import drive\n","\n","# === Configuration ===\n","STILLS_DIR = Path('/content/drive/MyDrive/FreeFuse_Project/Extracted_Stills')\n","OUTPUT_CSV = STILLS_DIR / 'draft_annotations.csv'\n","MODEL_NAME = \"facebook/detr-resnet-50\"\n","MAX_OBJS = 8\n","CONF_THRESH = 0.9\n","\n","# === Helper Functions ===\n","def mount_drive():\n","    \"\"\"Mount Google Drive.\"\"\"\n","    print(\"Mounting Google Drive...\")\n","    drive.mount('/content/drive', force_remount=True)\n","    print(\"Drive mounted.\")\n","\n","def parse_filename(fn: str):\n","    \"\"\"\n","    From image filename of form \"<video_id>_<MMSS>.jpg\", extract:\n","      - video_id (str)\n","      - timestamp_sec (int)\n","    \"\"\"\n","    stem = Path(fn).stem\n","    parts = stem.rsplit('_', 1)\n","    if len(parts) != 2 or not parts[1].isdigit():\n","        return stem, None\n","    vid, t = parts\n","    mm = int(t[:2]); ss = int(t[2:])\n","    return vid, mm*60 + ss\n","\n","# === Main Processing ===\n","mount_drive()\n","\n","# Load model and processor\n","print(f\"Loading model and processor ({MODEL_NAME})...\")\n","processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n","model = AutoModelForObjectDetection.from_pretrained(MODEL_NAME).to(\n","    torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",")\n","print(\"Model ready.\\n\")\n","\n","# Gather images\n","stills = sorted(f for f in STILLS_DIR.iterdir() if f.suffix.lower() in {'.jpg','.jpeg','.png'})\n","if not stills:\n","    print(f\"No stills found in {STILLS_DIR}\")\n","else:\n","    print(f\"Found {len(stills)} images to annotate.\\n\")\n","\n","    annotations = []\n","    for idx, img_path in enumerate(stills, start=1):\n","        print(f\"[{idx}/{len(stills)}] Processing '{img_path.name}'\")\n","        try:\n","            img = Image.open(img_path).convert(\"RGB\")\n","        except Exception as e:\n","            print(f\"  ✗ Failed to open image: {e}\")\n","            continue\n","\n","        width, height = img.size\n","        video_id, ts = parse_filename(img_path.name)\n","\n","        # Prepare tensor\n","        inputs = processor(images=img, return_tensors=\"pt\").to(model.device)\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","\n","        # Postprocess\n","        target_sizes = torch.tensor([[height, width]], device=model.device)\n","        results = processor.post_process_object_detection(\n","            outputs, threshold=CONF_THRESH, target_sizes=target_sizes\n","        )[0]\n","\n","        # Extract detections\n","        for j, (score, label_id, box) in enumerate(\n","            zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"])\n","        ):\n","            if j >= MAX_OBJS:\n","                break\n","            score_v = score.item()\n","            # Box coords [xmin, ymin, xmax, ymax] in pixels\n","            x_min, y_min, x_max, y_max = box.tolist()\n","            # Normalized\n","            x_min_norm = x_min / width\n","            y_min_norm = y_min / height\n","            x_max_norm = x_max / width\n","            y_max_norm = y_max / height\n","            # Derived\n","            bb_area = int((x_max - x_min) * (y_max - y_min))\n","\n","            annotations.append({\n","                'video_id': video_id,\n","                'image_file_name': img_path.name,\n","                'timestamp_sec': ts,\n","                'image_width_px': width,\n","                'image_height_px': height,\n","                'frame_id': img_path.stem,\n","                'object_id': f\"{img_path.stem}_obj{j+1}\",\n","                'class_id': label_id.item(),\n","                'object_name': model.config.id2label[label_id.item()],\n","                'object_category': 'N/A',\n","                'x_min_norm': round(x_min_norm, 4),\n","                'y_min_norm': round(y_min_norm, 4),\n","                'x_max_norm': round(x_max_norm, 4),\n","                'y_max_norm': round(y_max_norm, 4),\n","                'x_min': int(x_min),\n","                'y_min': int(y_min),\n","                'x_max': int(x_max),\n","                'y_max': int(y_max),\n","                'bb_area_px': bb_area,\n","                'confidence': round(score_v, 4),\n","                'frame_type': None,\n","                'interaction_score': None,\n","                'review_status': None,\n","                'reviewer_notes': None\n","            })\n","\n","    # Save to CSV\n","    if annotations:\n","        print(\"\\nSaving annotations to CSV...\")\n","        df = pd.DataFrame(annotations)\n","        cols = [\n","            'video_id','image_file_name','timestamp_sec','image_width_px','image_height_px',\n","            'frame_id','object_id','class_id','object_name','object_category',\n","            'x_min_norm','y_min_norm','x_max_norm','y_max_norm',\n","            'x_min','y_min','x_max','y_max',\n","            'bb_area_px','confidence',\n","            'frame_type','interaction_score',\n","            'review_status','reviewer_notes'\n","        ]\n","        df.to_csv(OUTPUT_CSV, index=False, columns=cols)\n","        print(f\"Annotations saved to {OUTPUT_CSV}\")\n","    else:\n","        print(\"No detections above threshold.\")"],"metadata":{"id":"FEUwjlB3JpAh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3) Label Stills"],"metadata":{"id":"SG4UZ4sDQ8hJ"}},{"cell_type":"code","source":["from pathlib import Path\n","import pandas as pd\n","import cv2\n","from google.colab import drive\n","\n","# === Configuration ===\n","STILLS_DIR = Path('/content/drive/MyDrive/FreeFuse_Project/Extracted_Stills')\n","ANNOTATIONS_CSV = STILLS_DIR / 'draft_annotations.csv'\n","OUTPUT_DIR = Path('/content/drive/MyDrive/FreeFuse_Project/Labeled_Stills')\n","\n","BOX_COLOR = (0, 255, 0)       # BGR\n","TEXT_COLOR = (255, 255, 255)  # BGR\n","BOX_THICKNESS = 2\n","FONT = cv2.FONT_HERSHEY_SIMPLEX\n","FONT_SCALE = 0.6\n","LINE_TYPE = cv2.LINE_AA\n","\n","# === Helper Functions ===\n","def mount_drive():\n","    \"\"\"Mount Google Drive.\"\"\"\n","    print(\"Mounting Google Drive...\")\n","    drive.mount('/content/drive', force_remount=True)\n","    print(\"Drive mounted.\")\n","\n","def load_annotations(csv_path: Path) -> pd.DataFrame:\n","    \"\"\"Load and validate annotation CSV.\"\"\"\n","    if not csv_path.exists():\n","        raise FileNotFoundError(f\"Annotation file not found: {csv_path}\")\n","    df = pd.read_csv(csv_path)\n","    required_cols = {\n","        'video_id','image_file_name','timestamp_sec','image_width_px','image_height_px',\n","        'frame_id','object_id','object_name','object_category',\n","        'x_min_norm','y_min_norm','x_max_norm','y_max_norm',\n","        'x_min','y_min','x_max','y_max','bb_area_px','bb_aspect_ratio',\n","        'confidence'\n","    }\n","    missing = required_cols - set(df.columns)\n","    if missing:\n","        raise ValueError(f\"Missing columns in CSV: {missing}\")\n","    return df\n","\n","def annotate_and_save(df: pd.DataFrame, stills_dir: Path, output_dir: Path):\n","    \"\"\"Draw bounding boxes on images and save annotated copies.\"\"\"\n","    output_dir.mkdir(parents=True, exist_ok=True)\n","    grouped = df.groupby('image_file_name')\n","    print(f\"Found annotations for {len(grouped)} images.\")\n","    for idx, (img_name, group) in enumerate(grouped, start=1):\n","        print(f\"[{idx}/{len(grouped)}] Annotating {img_name}\")\n","        img_path = stills_dir / img_name\n","        if not img_path.exists():\n","            print(f\"  ✗ Image not found: {img_path}\")\n","            continue\n","        image = cv2.imread(str(img_path))\n","        if image is None:\n","            print(f\"  ✗ Failed to load image: {img_path}\")\n","            continue\n","\n","        # Draw each annotation\n","        for _, row in group.iterrows():\n","            x_min = int(row['x_min'])\n","            y_min = int(row['y_min'])\n","            x_max = int(row['x_max'])\n","            y_max = int(row['y_max'])\n","            label = row['object_name']\n","            conf = row['confidence']\n","            # Draw bounding box\n","            cv2.rectangle(image, (x_min, y_min), (x_max, y_max), BOX_COLOR, BOX_THICKNESS)\n","            # Text background\n","            text = f\"{label}: {conf:.2f}\"\n","            (w, h), _ = cv2.getTextSize(text, FONT, FONT_SCALE, 1)\n","            cv2.rectangle(image, (x_min, y_min - h - 4), (x_min + w, y_min), BOX_COLOR, -1)\n","            # Text overlay\n","            cv2.putText(image, text, (x_min, y_min - 2), FONT, FONT_SCALE, TEXT_COLOR, 1, LINE_TYPE)\n","\n","        # Save annotated image\n","        out_name = img_path.stem + '_annotated.jpg'\n","        out_path = output_dir / out_name\n","        cv2.imwrite(str(out_path), image)\n","    print(\"Annotation of images complete.\")\n","\n","# === Main Execution ===\n","mount_drive()\n","try:\n","    annotations_df = load_annotations(ANNOTATIONS_CSV)\n","    annotate_and_save(annotations_df, STILLS_DIR, OUTPUT_DIR)\n","except Exception as e:\n","    print(f\"ERROR: {e}\")\n"],"metadata":{"id":"WmC0zzZaPf1p"},"execution_count":null,"outputs":[]}]}