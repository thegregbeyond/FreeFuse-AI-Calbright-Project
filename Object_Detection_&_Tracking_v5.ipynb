{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSJ7HGNK3kUppDbGvpPHcy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thegregbeyond/FreeFuse-AI-Calbright-Project/blob/main/Object_Detection_%26_Tracking_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **A) Combined YOLO + DeepSORT**\n",
        "\n"
      ],
      "metadata": {
        "id": "_MNeFFNT-bWP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lkZ36ICG4svF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e76325b-17c6-4d72-fa19-807ce4e97879"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.161)\n",
            "Requirement already satisfied: deep-sort-realtime in /usr/local/lib/python3.11/dist-packages (1.3.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.6.15)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Mounted at /content/drive\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-seg.pt to 'yolov8n-seg.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.74M/6.74M [00:00<00:00, 107MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 384x640 (no detections), 420.7ms\n",
            "Speed: 11.7ms preprocess, 420.7ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 252.3ms\n",
            "Speed: 4.6ms preprocess, 252.3ms inference, 30.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 1 motorcycle, 186.9ms\n",
            "Speed: 4.8ms preprocess, 186.9ms inference, 18.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 200.1ms\n",
            "Speed: 4.6ms preprocess, 200.1ms inference, 7.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 191.5ms\n",
            "Speed: 5.4ms preprocess, 191.5ms inference, 14.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 210.4ms\n",
            "Speed: 4.2ms preprocess, 210.4ms inference, 12.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 263.5ms\n",
            "Speed: 7.4ms preprocess, 263.5ms inference, 19.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 307.1ms\n",
            "Speed: 4.3ms preprocess, 307.1ms inference, 22.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 277.0ms\n",
            "Speed: 8.2ms preprocess, 277.0ms inference, 19.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 307.6ms\n",
            "Speed: 11.5ms preprocess, 307.6ms inference, 20.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 298.8ms\n",
            "Speed: 4.6ms preprocess, 298.8ms inference, 24.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 traffic light, 248.7ms\n",
            "Speed: 4.7ms preprocess, 248.7ms inference, 12.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 traffic light, 200.6ms\n",
            "Speed: 5.9ms preprocess, 200.6ms inference, 15.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 192.2ms\n",
            "Speed: 4.8ms preprocess, 192.2ms inference, 14.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 198.3ms\n",
            "Speed: 4.4ms preprocess, 198.3ms inference, 12.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 196.9ms\n",
            "Speed: 4.2ms preprocess, 196.9ms inference, 7.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 194.7ms\n",
            "Speed: 4.4ms preprocess, 194.7ms inference, 7.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 188.9ms\n",
            "Speed: 4.8ms preprocess, 188.9ms inference, 14.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 1 traffic light, 1 bottle, 1 cell phone, 191.7ms\n",
            "Speed: 5.0ms preprocess, 191.7ms inference, 29.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 traffic light, 1 bottle, 194.8ms\n",
            "Speed: 11.2ms preprocess, 194.8ms inference, 15.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 traffic light, 183.7ms\n",
            "Speed: 4.5ms preprocess, 183.7ms inference, 12.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 3 traffic lights, 195.4ms\n",
            "Speed: 4.7ms preprocess, 195.4ms inference, 14.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 184.2ms\n",
            "Speed: 4.7ms preprocess, 184.2ms inference, 14.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 umbrella, 1 bottle, 206.9ms\n",
            "Speed: 4.8ms preprocess, 206.9ms inference, 15.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 umbrella, 1 handbag, 184.6ms\n",
            "Speed: 5.0ms preprocess, 184.6ms inference, 17.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 handbag, 202.7ms\n",
            "Speed: 6.9ms preprocess, 202.7ms inference, 13.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 184.9ms\n",
            "Speed: 4.7ms preprocess, 184.9ms inference, 14.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 192.5ms\n",
            "Speed: 4.8ms preprocess, 192.5ms inference, 14.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 178.9ms\n",
            "Speed: 5.0ms preprocess, 178.9ms inference, 16.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 192.4ms\n",
            "Speed: 4.7ms preprocess, 192.4ms inference, 13.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 tennis racket, 298.3ms\n",
            "Speed: 5.2ms preprocess, 298.3ms inference, 22.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 tennis racket, 320.6ms\n",
            "Speed: 4.7ms preprocess, 320.6ms inference, 18.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 320.1ms\n",
            "Speed: 5.2ms preprocess, 320.1ms inference, 20.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 318.9ms\n",
            "Speed: 4.8ms preprocess, 318.9ms inference, 32.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 282.3ms\n",
            "Speed: 4.5ms preprocess, 282.3ms inference, 19.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 196.0ms\n",
            "Speed: 10.3ms preprocess, 196.0ms inference, 12.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 tennis racket, 180.4ms\n",
            "Speed: 5.3ms preprocess, 180.4ms inference, 14.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 199.1ms\n",
            "Speed: 5.6ms preprocess, 199.1ms inference, 12.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 206.8ms\n",
            "Speed: 5.1ms preprocess, 206.8ms inference, 12.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 handbag, 194.0ms\n",
            "Speed: 5.1ms preprocess, 194.0ms inference, 12.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 200.4ms\n",
            "Speed: 5.4ms preprocess, 200.4ms inference, 14.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 196.6ms\n",
            "Speed: 4.8ms preprocess, 196.6ms inference, 14.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 202.8ms\n",
            "Speed: 4.4ms preprocess, 202.8ms inference, 13.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 192.4ms\n",
            "Speed: 4.7ms preprocess, 192.4ms inference, 13.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 191.8ms\n",
            "Speed: 5.6ms preprocess, 191.8ms inference, 14.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 191.5ms\n",
            "Speed: 4.8ms preprocess, 191.5ms inference, 14.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 parking meter, 187.0ms\n",
            "Speed: 6.8ms preprocess, 187.0ms inference, 11.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 200.1ms\n",
            "Speed: 6.4ms preprocess, 200.1ms inference, 14.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 182.5ms\n",
            "Speed: 5.1ms preprocess, 182.5ms inference, 16.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 207.4ms\n",
            "Speed: 4.9ms preprocess, 207.4ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 1 umbrella, 1 frisbee, 194.8ms\n",
            "Speed: 4.8ms preprocess, 194.8ms inference, 12.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 1 frisbee, 193.5ms\n",
            "Speed: 6.4ms preprocess, 193.5ms inference, 14.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 1 frisbee, 178.7ms\n",
            "Speed: 5.4ms preprocess, 178.7ms inference, 13.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 325.0ms\n",
            "Speed: 9.0ms preprocess, 325.0ms inference, 11.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 1 umbrella, 275.9ms\n",
            "Speed: 6.6ms preprocess, 275.9ms inference, 22.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 1 umbrella, 301.2ms\n",
            "Speed: 7.5ms preprocess, 301.2ms inference, 20.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 1 umbrella, 300.3ms\n",
            "Speed: 10.0ms preprocess, 300.3ms inference, 25.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 umbrella, 324.0ms\n",
            "Speed: 4.9ms preprocess, 324.0ms inference, 22.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 291.8ms\n",
            "Speed: 7.7ms preprocess, 291.8ms inference, 11.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 194.5ms\n",
            "Speed: 5.3ms preprocess, 194.5ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 182.6ms\n",
            "Speed: 3.6ms preprocess, 182.6ms inference, 7.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 192.6ms\n",
            "Speed: 4.7ms preprocess, 192.6ms inference, 13.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 1 umbrella, 1 bottle, 181.5ms\n",
            "Speed: 5.1ms preprocess, 181.5ms inference, 12.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 188.2ms\n",
            "Speed: 4.6ms preprocess, 188.2ms inference, 13.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 181.5ms\n",
            "Speed: 5.0ms preprocess, 181.5ms inference, 12.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 193.7ms\n",
            "Speed: 4.6ms preprocess, 193.7ms inference, 7.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 191.0ms\n",
            "Speed: 4.8ms preprocess, 191.0ms inference, 7.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 199.1ms\n",
            "Speed: 6.0ms preprocess, 199.1ms inference, 7.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 182.3ms\n",
            "Speed: 4.7ms preprocess, 182.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 184.2ms\n",
            "Speed: 5.4ms preprocess, 184.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 185.1ms\n",
            "Speed: 4.7ms preprocess, 185.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 198.1ms\n",
            "Speed: 4.3ms preprocess, 198.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 188.6ms\n",
            "Speed: 4.5ms preprocess, 188.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 185.1ms\n",
            "Speed: 4.9ms preprocess, 185.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 186.0ms\n",
            "Speed: 4.3ms preprocess, 186.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 193.9ms\n",
            "Speed: 6.8ms preprocess, 193.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 181.7ms\n",
            "Speed: 4.7ms preprocess, 181.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 181.4ms\n",
            "Speed: 4.4ms preprocess, 181.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 197.6ms\n",
            "Speed: 4.8ms preprocess, 197.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 183.8ms\n",
            "Speed: 3.5ms preprocess, 183.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 181.6ms\n",
            "Speed: 4.8ms preprocess, 181.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 189.9ms\n",
            "Speed: 5.1ms preprocess, 189.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 203.8ms\n",
            "Speed: 4.9ms preprocess, 203.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 180.1ms\n",
            "Speed: 5.1ms preprocess, 180.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 181.5ms\n",
            "Speed: 4.4ms preprocess, 181.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 189.4ms\n",
            "Speed: 6.3ms preprocess, 189.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Saved annotated video(s) to /content/drive/MyDrive/FreeFuse_Project/Videos/Output\n",
            "Saved annotations to /content/drive/MyDrive/FreeFuse_Project/Videos/Output/draft_annotations.csv\n"
          ]
        }
      ],
      "source": [
        "# A) Combine YOLO + DeepSORT\n",
        "# === Install required packages (run once) ===\n",
        "!pip install ultralytics deep-sort-realtime opencv-python pandas\n",
        "\n",
        "from google.colab import drive\n",
        "from ultralytics import YOLO\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "import cv2\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "# === Parameters ===\n",
        "VIDEO_FOLDER         = Path(\"/content/drive/MyDrive/FreeFuse_Project/Videos/Input\")\n",
        "OUTPUT_FOLDER        = Path(\"/content/drive/MyDrive/FreeFuse_Project/Videos/Output\")\n",
        "CONFIDENCE_THRESHOLD = 0.5\n",
        "DETECTION_INTERVAL   = 5     # analyze every Nth frame\n",
        "MAX_TRACK_AGE        = 30     # frames to keep a lost track\n",
        "MIN_HITS             = 3      # detections before confirming a track\n",
        "\n",
        "# drawing settings\n",
        "MASK_COLOR           = (0, 255, 0)    # BGR mask outline color\n",
        "MASK_THICKNESS       = 2              # mask polygon line thickness\n",
        "TEXT_COLOR           = (255, 255, 255)# BGR text color\n",
        "TEXT_FONT            = cv2.FONT_HERSHEY_SIMPLEX\n",
        "TEXT_SCALE           = 0.6\n",
        "TEXT_THICKNESS       = 2\n",
        "\n",
        "# === 1) Mount Google Drive ===\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# === 2) Load YOLOv8-nano segmentation & DeepSORT ===\n",
        "model   = YOLO('yolov8n-seg')           # auto-downloads nano-segmentation weights\n",
        "tracker = DeepSort(max_age=MAX_TRACK_AGE, n_init=MIN_HITS)\n",
        "\n",
        "# utility to compute IoU between two boxes\n",
        "\n",
        "def compute_iou(boxA, boxB):\n",
        "    xA1,yA1,xA2,yA2 = boxA\n",
        "    xB1,yB1,xB2,yB2 = boxB\n",
        "    xi1, yi1 = max(xA1,xB1), max(yA1,yB1)\n",
        "    xi2, yi2 = min(xA2,xB2), min(yA2,yB2)\n",
        "    inter = max(0, xi2-xi1) * max(0, yi2-yi1)\n",
        "    union = (xA2-xA1)*(yA2-yA1) + (xB2-xB1)*(yB2-yB1) - inter\n",
        "    return inter/union if union>0 else 0\n",
        "\n",
        "annotations = []\n",
        "\n",
        "# ensure output CSV and video folder exist\n",
        "OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for video_file in os.listdir(VIDEO_FOLDER):\n",
        "    if not video_file.lower().endswith(('.mp4','.mov','.avi')):\n",
        "        continue\n",
        "\n",
        "    cap        = cv2.VideoCapture(str(VIDEO_FOLDER/video_file))\n",
        "    fps        = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width      = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height     = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    frame_num  = 0\n",
        "    video_name = Path(video_file).stem\n",
        "\n",
        "    # prepare video writer\n",
        "    output_path = OUTPUT_FOLDER / video_file\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    writer = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_num % DETECTION_INTERVAL == 0:\n",
        "            timestamp_sec = int(frame_num / fps)\n",
        "            frame_id      = f\"{video_name}_{timestamp_sec:04d}\"\n",
        "\n",
        "            # YOLOv8 segmentation inference\n",
        "            results = model(frame)[0]\n",
        "\n",
        "            dets_for_tracker = []\n",
        "            det_meta = []\n",
        "            for idx, (box, score, cls) in enumerate(zip(\n",
        "                    results.boxes.xyxy, results.boxes.conf, results.boxes.cls)):\n",
        "                conf = float(score)\n",
        "                if conf < CONFIDENCE_THRESHOLD:\n",
        "                    continue\n",
        "\n",
        "                x1,y1,x2,y2 = box.cpu().numpy().astype(int)\n",
        "                cls_id      = int(cls.cpu().numpy())\n",
        "                name        = model.names[cls_id]\n",
        "\n",
        "                # extract polygon in original image scale\n",
        "                # YOLOv8 provides masks.xy which are already scaled\n",
        "                poly = np.array(results.masks.xy[idx], dtype=np.int32)\n",
        "                # ensure shape (-1,2)\n",
        "                poly = poly.reshape(-1,2)\n",
        "\n",
        "                dets_for_tracker.append([[x1,y1,x2-x1,y2-y1], conf, name])\n",
        "                det_meta.append({\n",
        "                    \"bbox\": (x1,y1,x2,y2),\n",
        "                    \"MID\": f\"/m/{cls_id:07d}\",\n",
        "                    \"object_name\": name,\n",
        "                    \"object_category\": \"unknown\",\n",
        "                    \"mask_poly\": poly.tolist(),\n",
        "                    \"confidence\": conf,\n",
        "                })\n",
        "\n",
        "            # update tracker\n",
        "            tracks = tracker.update_tracks(dets_for_tracker, frame=frame)\n",
        "\n",
        "            if det_meta:\n",
        "                for trk in tracks:\n",
        "                    if not trk.is_confirmed():\n",
        "                        continue\n",
        "                    tx1,ty1,tx2,ty2 = trk.to_tlbr()\n",
        "                    track_id = trk.track_id\n",
        "\n",
        "                    # match detection by IoU\n",
        "                    best_iou, best = max(\n",
        "                        ((compute_iou((tx1,ty1,tx2,ty2), m[\"bbox\"]), m) for m in det_meta),\n",
        "                        key=lambda x: x[0]\n",
        "                    )\n",
        "                    if best_iou > 0.3:\n",
        "                        # draw mask outline using original-scale polygon\n",
        "                        pts = np.array(best[\"mask_poly\"], np.int32)\n",
        "                        if pts.size:\n",
        "                            cv2.polylines(frame, [pts], isClosed=True, color=MASK_COLOR, thickness=MASK_THICKNESS)\n",
        "                            # place label at first vertex\n",
        "                            label_pos = tuple(pts[0])\n",
        "                            cv2.putText(frame, best[\"object_name\"], label_pos, TEXT_FONT,\n",
        "                                        TEXT_SCALE, TEXT_COLOR, TEXT_THICKNESS, cv2.LINE_AA)\n",
        "\n",
        "                        # record annotation\n",
        "                        annotations.append({\n",
        "                            \"video_filename\":    video_file,\n",
        "                            \"frame_id\":          frame_id,\n",
        "                            \"track_id\":          f\"{video_name}_{track_id}\",\n",
        "                            \"object_id\":         f\"{frame_id}_obj{track_id}\",\n",
        "                            \"timestamp_sec\":     timestamp_sec,\n",
        "                            \"image_width_px\":    width,\n",
        "                            \"image_height_px\":   height,\n",
        "                            \"MID\":               best[\"MID\"],\n",
        "                            \"object_name\":       best[\"object_name\"],\n",
        "                            \"object_category\":   best[\"object_category\"],\n",
        "                            \"x_min\":             int(tx1),\n",
        "                            \"y_min\":             int(ty1),\n",
        "                            \"x_max\":             int(tx2),\n",
        "                            \"y_max\":             int(ty2),\n",
        "                            \"segmentation_mask\": json.dumps([best[\"mask_poly\"]]),\n",
        "                            \"confidence\":        best[\"confidence\"],\n",
        "                            \"interaction_score\": 0.0\n",
        "                        })\n",
        "\n",
        "        # write frame (with masks) to output\n",
        "        writer.write(frame)\n",
        "        frame_num += 1\n",
        "\n",
        "    cap.release()\n",
        "    writer.release()\n",
        "\n",
        "# write CSV of annotations\n",
        "out_csv = OUTPUT_FOLDER / \"draft_annotations.csv\"\n",
        "pd.DataFrame(annotations).to_csv(out_csv, index=False)\n",
        "print(f\"Saved annotated video(s) to {OUTPUT_FOLDER}\")\n",
        "print(f\"Saved annotations to {out_csv}\")"
      ]
    }
  ]
}